{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"0aStgWSO0E0E\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Cluster\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"1eLEkw5O0ECa\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Objectives\\n\",\n",
    "    \"\\n\",\n",
    "    \"* Fit and evaluate a cluster model to group similar data\\n\",\n",
    "    \"* Understand the profile for each cluster\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Inputs\\n\",\n",
    "    \"\\n\",\n",
    "    \"* outputs/datasets/collection/TelcoCustomerChurn.csv\\n\",\n",
    "    \"* Instructions on which variables to use for data cleaning and feature engineering. They are found in their respective notebooks.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Outputs\\n\",\n",
    "    \"\\n\",\n",
    "    \"* Cluster Pipeline\\n\",\n",
    "    \"* Train Set\\n\",\n",
    "    \"* Most important features to define a cluster plot\\n\",\n",
    "    \"* Clusters Profile Description\\n\",\n",
    "    \"* Cluster Silhouette\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"9uWZXH9LwoQg\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"---\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Change working directory\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"We need to change the working directory from its current folder to its parent folder\\n\",\n",
    "    \"* We access the current directory with os.getcwd()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"current_dir = os.getcwd()\\n\",\n",
    "    \"current_dir\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"We want to make the parent of the parent of current directory the new current directory\\n\",\n",
    "    \"* os.path.dirname() gets the parent directory\\n\",\n",
    "    \"* os.chir() defines the new current directory\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"os.chdir(os.path.dirname(current_dir))\\n\",\n",
    "    \"print(\\\"You set a new current directory\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Confirm the new current directory\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"current_dir = os.getcwd()\\n\",\n",
    "    \"current_dir\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"pXKlJFX0iuM5\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"---\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"-mavJ8DibrcQ\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Load Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"Xk7DU_ekbtX8\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"df = (pd.read_csv(\\\"outputs/datasets/collection/TelcoCustomerChurn.csv\\\")\\n\",\n",
    "    \"      .drop(['customerID', 'TotalCharges', 'Churn', 'tenure'], axis=1)\\n\",\n",
    "    \"      )\\n\",\n",
    "    \"print(df.shape)\\n\",\n",
    "    \"df.head(3)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"krjAk78Tbyhv\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Cluster Pipeline with all data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"NZWZHhpYaDjf\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"##  ML Cluster Pipeline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"C6keis6ao8LA\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.pipeline import Pipeline\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Feature Engineering\\n\",\n",
    "    \"from feature_engine.encoding import OrdinalEncoder\\n\",\n",
    "    \"from feature_engine.selection import SmartCorrelatedSelection\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Feat Scaling\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
    "    \"\\n\",\n",
    "    \"# PCA\\n\",\n",
    "    \"from sklearn.decomposition import PCA\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ML algorithm\\n\",\n",
    "    \"from sklearn.cluster import KMeans\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def PipelineCluster():\\n\",\n",
    "    \"    pipeline_base = Pipeline([\\n\",\n",
    "    \"        (\\\"OrdinalCategoricalEncoder\\\", OrdinalEncoder(encoding_method='arbitrary',\\n\",\n",
    "    \"                                                     variables=['gender', 'Partner', 'Dependents', 'PhoneService',\\n\",\n",
    "    \"                                                                'MultipleLines', 'InternetService', 'OnlineSecurity',\\n\",\n",
    "    \"                                                                'OnlineBackup', 'DeviceProtection', 'TechSupport',\\n\",\n",
    "    \"                                                                'StreamingTV', 'StreamingMovies', 'Contract',\\n\",\n",
    "    \"                                                                'PaperlessBilling', 'PaymentMethod'])),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"SmartCorrelatedSelection\\\", SmartCorrelatedSelection(variables=None, method=\\\"spearman\\\",\\n\",\n",
    "    \"                                                              threshold=0.6, selection_method=\\\"variance\\\")),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"scaler\\\", StandardScaler()),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"PCA\\\", PCA(n_components=50, random_state=0)),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"model\\\", KMeans(n_clusters=50, random_state=0)),\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    ])\\n\",\n",
    "    \"    return pipeline_base\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"Mrr31sD9DyvY\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Principal Component Analysis (PCA)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"es49S65qqvRw\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"pipeline_cluster = PipelineCluster()\\n\",\n",
    "    \"pipeline_pca = Pipeline(pipeline_cluster.steps[:-2])\\n\",\n",
    "    \"df_pca = pipeline_pca.fit_transform(df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(df_pca.shape,'\\\\n', type(df_pca))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"WlABEj9Iw6Jr\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Apply PCA separately to the scaled data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"cM_Xsqxsrt5M\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"sns.set_style(\\\"whitegrid\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"n_components = 12\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def pca_components_analysis(df_pca, n_components):\\n\",\n",
    "    \"    pca = PCA(n_components=n_components).fit(df_pca)\\n\",\n",
    "    \"    x_PCA = pca.transform(df_pca)  # array with transformed PCA\\n\",\n",
    "    \"\\n\",\n",
    "    \"    ComponentsList = [\\\"Component \\\" + str(number)\\n\",\n",
    "    \"                      for number in range(n_components)]\\n\",\n",
    "    \"    dfExplVarRatio = pd.DataFrame(\\n\",\n",
    "    \"        data=np.round(100 * pca.explained_variance_ratio_, 3),\\n\",\n",
    "    \"        index=ComponentsList,\\n\",\n",
    "    \"        columns=['Explained Variance Ratio (%)'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"    dfExplVarRatio['Accumulated Variance'] = dfExplVarRatio['Explained Variance Ratio (%)'].cumsum(\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum(\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print(\\n\",\n",
    "    \"        f\\\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\\\n\\\")\\n\",\n",
    "    \"    plt.figure(figsize=(9, 6))\\n\",\n",
    "    \"    sns.lineplot(data=dfExplVarRatio,  marker=\\\"o\\\")\\n\",\n",
    "    \"    plt.xticks(rotation=90)\\n\",\n",
    "    \"    plt.yticks(np.arange(0, 110, 10))\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"pca_components_analysis(df_pca=df_pca, n_components=n_components)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"pca_components_analysis(df_pca=df_pca,n_components=6)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def PipelineCluster():\\n\",\n",
    "    \"    pipeline_base = Pipeline([\\n\",\n",
    "    \"        (\\\"OrdinalCategoricalEncoder\\\", OrdinalEncoder(encoding_method='arbitrary',\\n\",\n",
    "    \"                                                     variables=['gender', 'Partner', 'Dependents', 'PhoneService',\\n\",\n",
    "    \"                                                                'MultipleLines', 'InternetService', 'OnlineSecurity',\\n\",\n",
    "    \"                                                                'OnlineBackup', 'DeviceProtection', 'TechSupport',\\n\",\n",
    "    \"                                                                'StreamingTV', 'StreamingMovies', 'Contract',\\n\",\n",
    "    \"                                                                'PaperlessBilling', 'PaymentMethod'])),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"SmartCorrelatedSelection\\\", SmartCorrelatedSelection(variables=None, method=\\\"spearman\\\",\\n\",\n",
    "    \"                                                              threshold=0.6, selection_method=\\\"variance\\\")),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"scaler\\\", StandardScaler()),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # we update n_components to 6\\n\",\n",
    "    \"        (\\\"PCA\\\", PCA(n_components=6, random_state=0)),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"model\\\", KMeans(n_clusters=50, random_state=0)),\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    ])\\n\",\n",
    "    \"    return pipeline_base\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"PipelineCluster()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"Uw9NtDj4EtEJ\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Elbow Method and Silhouette Score\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"JVaMnb9vGyBw\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"pipeline_cluster = PipelineCluster()\\n\",\n",
    "    \"pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\\n\",\n",
    "    \"df_analysis = pipeline_analysis.fit_transform(df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(df_analysis.shape,'\\\\n', type(df_analysis))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"TZBcHjt7EwFT\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from yellowbrick.cluster import KElbowVisualizer\\n\",\n",
    "    \"\\n\",\n",
    "    \"visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11)) # 11 is not inclusive, it will plot until 10\\n\",\n",
    "    \"visualizer.fit(df_analysis) \\n\",\n",
    "    \"visualizer.show() \\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from yellowbrick.cluster import SilhouetteVisualizer\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 6 is not inclusive, it will stop at 5\\n\",\n",
    "    \"n_cluster_start, n_cluster_stop = 2, 6\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"=== Average Silhouette Score for different number of clusters ===\\\")\\n\",\n",
    "    \"visualizer = KElbowVisualizer(KMeans(random_state=0), k=(\\n\",\n",
    "    \"    n_cluster_start, n_cluster_stop), metric='silhouette')\\n\",\n",
    "    \"visualizer.fit(df_analysis)\\n\",\n",
    "    \"visualizer.show()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"print(\\\"\\\\n\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print(f\\\"=== Silhouette plot for {n_clusters} Clusters ===\\\")\\n\",\n",
    "    \"    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters, random_state=0),\\n\",\n",
    "    \"                                      colors='yellowbrick')\\n\",\n",
    "    \"    visualizer.fit(df_analysis)\\n\",\n",
    "    \"    visualizer.show()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    print(\\\"\\\\n\\\")\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def PipelineCluster():\\n\",\n",
    "    \"    pipeline_base = Pipeline([\\n\",\n",
    "    \"        (\\\"OrdinalCategoricalEncoder\\\", OrdinalEncoder(encoding_method='arbitrary',\\n\",\n",
    "    \"                                                     variables=['gender', 'Partner', 'Dependents', 'PhoneService',\\n\",\n",
    "    \"                                                                'MultipleLines', 'InternetService', 'OnlineSecurity',\\n\",\n",
    "    \"                                                                'OnlineBackup', 'DeviceProtection', 'TechSupport',\\n\",\n",
    "    \"                                                                'StreamingTV', 'StreamingMovies', 'Contract',\\n\",\n",
    "    \"                                                                'PaperlessBilling', 'PaymentMethod'])),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"SmartCorrelatedSelection\\\", SmartCorrelatedSelection(variables=None, method=\\\"spearman\\\",\\n\",\n",
    "    \"                                                              threshold=0.6, selection_method=\\\"variance\\\")),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"scaler\\\", StandardScaler()),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"PCA\\\", PCA(n_components=6, random_state=0)),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # we update n_clusters to 3\\n\",\n",
    "    \"        (\\\"model\\\", KMeans(n_clusters=3, random_state=0)),\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    ])\\n\",\n",
    "    \"    return pipeline_base\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"PipelineCluster()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"YQBjAlRsHhU4\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Fit Cluster Pipeline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"kpxaylKk-6CQ\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Quick recap of our data for training cluster pipeline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"zfKHc63v-6Zm\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"X = df.copy()\\n\",\n",
    "    \"print(X.shape)\\n\",\n",
    "    \"X.head(3)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"NfRpKC4Ykreg\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Fit Cluster pipeline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"MAiyUpTWHjQh\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"pipeline_cluster = PipelineCluster()\\n\",\n",
    "    \"pipeline_cluster.fit(X)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"0L0iMkjJHXSI\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Add cluster predictions to dataset\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"ZKT5IjmTmei8\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"We add a column \\\"`Clusters`\\\" (with the cluster pipeline predictions) to the dataset\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"ow8B0xVdmlgK\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"X['Clusters'] = pipeline_cluster['model'].labels_\\n\",\n",
    "    \"print(X.shape)\\n\",\n",
    "    \"X.head(3)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"eAVrYJEqxYyG\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(f\\\"* Clusters frequencies \\\\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\\\n\\\\n\\\")\\n\",\n",
    "    \"X['Clusters'].value_counts().sort_values().plot(kind='bar')\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"sns.set_style(\\\"whitegrid\\\")\\n\",\n",
    "    \"plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"sns.scatterplot(x=df_analysis[:, 0], y=df_analysis[:, 1],\\n\",\n",
    "    \"                hue=X['Clusters'], palette='Set1', alpha=0.6)\\n\",\n",
    "    \"plt.scatter(x=pipeline_cluster['model'].cluster_centers_[:, 0], y=pipeline_cluster['model'].cluster_centers_[:, 1],\\n\",\n",
    "    \"            marker=\\\"x\\\", s=169, linewidths=3, color=\\\"black\\\")\\n\",\n",
    "    \"plt.xlabel(\\\"PCA Component 0\\\")\\n\",\n",
    "    \"plt.ylabel(\\\"PCA Component 1\\\")\\n\",\n",
    "    \"plt.title(\\\"PCA Components colored by Clusters\\\")\\n\",\n",
    "    \"plt.show()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"MnjHhYjXng2r\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"We save the cluster predictions from this pipeline to use in the future. We will get back to that in a later stage.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"FWgb0kPOWtMa\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"cluster_predictions_with_all_variables = X['Clusters']\\n\",\n",
    "    \"cluster_predictions_with_all_variables\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"sTWTf1rgkQ7b\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Fit a classifier, where the target is cluster predictions and features remaining variables\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"hP6sGUn0XyDm\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"We copy `X` to a DataFrame `df_clf`\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"OeLq81sm2yAg\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_clf = X.copy()\\n\",\n",
    "    \"print(df_clf.shape)\\n\",\n",
    "    \"df_clf.head(3)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"4b3Ei6Os5X3s\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Split Train and Test sets\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"cgHXehCVyzUl\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.model_selection import train_test_split\\n\",\n",
    "    \"X_train, X_test, y_train, y_test = train_test_split(\\n\",\n",
    "    \"    df_clf.drop(['Clusters'], axis=1),\\n\",\n",
    "    \"    df_clf['Clusters'],\\n\",\n",
    "    \"    test_size=0.2,\\n\",\n",
    "    \"    random_state=0\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(X_train.shape, X_test.shape)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"6EZUk-uV5aN8\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Create classifier pipeline steps\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Feat Selection\\n\",\n",
    "    \"from sklearn.feature_selection import SelectFromModel\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ML algorithm\\n\",\n",
    "    \"from sklearn.ensemble import GradientBoostingClassifier\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def PipelineClf2ExplainClusters():\\n\",\n",
    "    \"    pipeline_base = Pipeline([\\n\",\n",
    "    \"        (\\\"OrdinalCategoricalEncoder\\\", OrdinalEncoder(encoding_method='arbitrary',\\n\",\n",
    "    \"                                                     variables=['gender', 'Partner', 'Dependents', 'PhoneService',\\n\",\n",
    "    \"                                                                'MultipleLines', 'InternetService', 'OnlineSecurity',\\n\",\n",
    "    \"                                                                'OnlineBackup', 'DeviceProtection', 'TechSupport',\\n\",\n",
    "    \"                                                                'StreamingTV', 'StreamingMovies', 'Contract',\\n\",\n",
    "    \"                                                                'PaperlessBilling', 'PaymentMethod'])),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"SmartCorrelatedSelection\\\", SmartCorrelatedSelection(variables=None, method=\\\"spearman\\\",\\n\",\n",
    "    \"                                                              threshold=0.6, selection_method=\\\"variance\\\")),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"scaler\\\", StandardScaler()),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"feat_selection\\\", SelectFromModel(\\n\",\n",
    "    \"            GradientBoostingClassifier(random_state=0))),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"model\\\", GradientBoostingClassifier(random_state=0)),\\n\",\n",
    "    \"\\n\",\n",
    "    \"    ])\\n\",\n",
    "    \"    return pipeline_base\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"PipelineClf2ExplainClusters()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Fit the classifier to the training data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"3R7xdg1Av0Ce\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"pipeline_clf_cluster = PipelineClf2ExplainClusters()\\n\",\n",
    "    \"pipeline_clf_cluster.fit(X_train, y_train)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"z05LMFoZ4T2K\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Evaluate classifier performance on Train and Test Sets\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"M1iqL2Kc544K\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.metrics import classification_report\\n\",\n",
    "    \"print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"0Oo4xJMZ615p\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"MEwjHBSh5ejG\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Assess the most important Features that define a cluster\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"BG5ztHxsKcd5\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# after data cleaning and feature engineering, the feature space changes\\n\",\n",
    "    \"\\n\",\n",
    "    \"# how many data cleaning and feature engineering steps does your pipeline have?\\n\",\n",
    "    \"data_cleaning_feat_eng_steps = 2\\n\",\n",
    "    \"columns_after_data_cleaning_feat_eng = (Pipeline(pipeline_clf_cluster.steps[:data_cleaning_feat_eng_steps])\\n\",\n",
    "    \"                                        .transform(X_train)\\n\",\n",
    "    \"                                        .columns)\\n\",\n",
    "    \"\\n\",\n",
    "    \"best_features = columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support(\\n\",\n",
    "    \")].to_list()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# create DataFrame to display feature importance\\n\",\n",
    "    \"df_feature_importance = (pd.DataFrame(data={\\n\",\n",
    "    \"    'Feature': columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()],\\n\",\n",
    "    \"    'Importance': pipeline_clf_cluster['model'].feature_importances_})\\n\",\n",
    "    \"    .sort_values(by='Importance', ascending=False)\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# reassign best features in importance order\\n\",\n",
    "    \"best_features = df_feature_importance['Feature'].to_list()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Most important features statement and plot\\n\",\n",
    "    \"print(f\\\"* These are the {len(best_features)} most important features in descending order. \\\"\\n\",\n",
    "    \"      f\\\"The model was trained on them: \\\\n{best_features} \\\\n\\\")\\n\",\n",
    "    \"df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\\n\",\n",
    "    \"plt.show()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"qgul0EF9nx_E\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"We will store the best_features to use at a later stage.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"YzyMkwHznyG8\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"best_features_pipeline_all_variables = best_features\\n\",\n",
    "    \"best_features_pipeline_all_variables\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"J2ywCxJmkRQn\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Cluster Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"hZMr-wiudEkb\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Load function that plots a table with description for all Clusters\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"_lpRVDqTdEul\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"def DescriptionAllClusters(df, decimal_points=3):\\n\",\n",
    "    \"\\n\",\n",
    "    \"    DescriptionAllClusters = pd.DataFrame(\\n\",\n",
    "    \"        columns=df.drop(['Clusters'], axis=1).columns)\\n\",\n",
    "    \"    # iterate on each cluster , calls Clusters_IndividualDescription()\\n\",\n",
    "    \"    for cluster in df.sort_values(by='Clusters')['Clusters'].unique():\\n\",\n",
    "    \"\\n\",\n",
    "    \"        EDA_ClusterSubset = df.query(\\n\",\n",
    "    \"            f\\\"Clusters == {cluster}\\\").drop(['Clusters'], axis=1)\\n\",\n",
    "    \"        ClusterDescription = Clusters_IndividualDescription(\\n\",\n",
    "    \"            EDA_ClusterSubset, cluster, decimal_points)\\n\",\n",
    "    \"        DescriptionAllClusters = pd.concat(\\n\",\n",
    "    \"            [ClusterDescription, DescriptionAllClusters])\\n\",\n",
    "    \"\\n\",\n",
    "    \"    DescriptionAllClusters.set_index(['Cluster'], inplace=True)\\n\",\n",
    "    \"    return DescriptionAllClusters\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def Clusters_IndividualDescription(EDA_Cluster, cluster, decimal_points):\\n\",\n",
    "    \"\\n\",\n",
    "    \"    ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\\n\",\n",
    "    \"    # for a given cluster, iterate over all columns\\n\",\n",
    "    \"    # if the variable is numerical, calculate the IQR: display as Q1 -- Q3.\\n\",\n",
    "    \"    # That will show the range for the most common values for the numerical variable\\n\",\n",
    "    \"    # if the variable is categorical, count the frequencies and displays the top 3 most frequent\\n\",\n",
    "    \"    # That will show the most common levels for the category\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for col in EDA_Cluster.columns:\\n\",\n",
    "    \"\\n\",\n",
    "    \"        try:  # eventually a given cluster will have only missing data for a given variable\\n\",\n",
    "    \"\\n\",\n",
    "    \"            if EDA_Cluster[col].dtypes == 'object':\\n\",\n",
    "    \"\\n\",\n",
    "    \"                top_frequencies = EDA_Cluster.dropna(\\n\",\n",
    "    \"                    subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\\n\",\n",
    "    \"                Description = ''\\n\",\n",
    "    \"\\n\",\n",
    "    \"                for x in range(len(top_frequencies)):\\n\",\n",
    "    \"                    freq = top_frequencies.iloc[x]\\n\",\n",
    "    \"                    category = top_frequencies.index[x][0]\\n\",\n",
    "    \"                    CategoryPercentage = int(round(freq*100, 0))\\n\",\n",
    "    \"                    statement = f\\\"'{category}': {CategoryPercentage}% , \\\"\\n\",\n",
    "    \"                    Description = Description + statement\\n\",\n",
    "    \"\\n\",\n",
    "    \"                ClustersDescription.at[0, col] = Description[:-2]\\n\",\n",
    "    \"\\n\",\n",
    "    \"            elif EDA_Cluster[col].dtypes in ['float', 'int']:\\n\",\n",
    "    \"                DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\\n\",\n",
    "    \"                Q1 = round(DescStats.iloc[4, 0], decimal_points)\\n\",\n",
    "    \"                Q3 = round(DescStats.iloc[6, 0], decimal_points)\\n\",\n",
    "    \"                Description = f\\\"{Q1} -- {Q3}\\\"\\n\",\n",
    "    \"                ClustersDescription.at[0, col] = Description\\n\",\n",
    "    \"\\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            ClustersDescription.at[0, col] = 'Not available'\\n\",\n",
    "    \"            print(\\n\",\n",
    "    \"                f\\\"** Error Exception: {e} - cluster {cluster}, variable {col}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    ClustersDescription['Cluster'] = str(cluster)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return ClustersDescription\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"OHo7wmH68AYc\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Load a custom function to plot cluster distribution per Variable (absolute and relative levels)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"NN23X2dT8AeA\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def cluster_distribution_per_variable(df, target):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    The data should have 2 variables, the cluster predictions and\\n\",\n",
    "    \"    the variable you want to analyze with, in this case we call \\\"target\\\".\\n\",\n",
    "    \"    We use plotly express to create 2 plots:\\n\",\n",
    "    \"    Cluster distribution across the target.\\n\",\n",
    "    \"    Relative presence of the target level in each cluster.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    df_bar_plot = df.groupby(['Clusters', target]).size().reset_index(name='Count')\\n\",\n",
    "    \"    df_bar_plot.columns = ['Clusters', target, 'Count']\\n\",\n",
    "    \"    df_bar_plot[target] = df_bar_plot[target].astype('object')\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print(f\\\"Clusters distribution across {target} levels\\\")\\n\",\n",
    "    \"    fig = px.bar(df_bar_plot, x='Clusters', y='Count',\\n\",\n",
    "    \"                 color=target, width=800, height=500)\\n\",\n",
    "    \"    fig.update_layout(xaxis=dict(tickmode='array',\\n\",\n",
    "    \"                      tickvals=df['Clusters'].unique()))\\n\",\n",
    "    \"    fig.show(renderer='jupyterlab')\\n\",\n",
    "    \"\\n\",\n",
    "    \"    df_relative = (df\\n\",\n",
    "    \"                   .groupby([\\\"Clusters\\\", target])\\n\",\n",
    "    \"                   .size()\\n\",\n",
    "    \"                   .unstack(fill_value=0)\\n\",\n",
    "    \"                   .apply(lambda x: 100 * x / x.sum(), axis=1)\\n\",\n",
    "    \"                   .stack()\\n\",\n",
    "    \"                   .reset_index(name='Relative Percentage (%)')\\n\",\n",
    "    \"                   .sort_values(by=['Clusters', target])\\n\",\n",
    "    \"                   )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print(f\\\"Relative Percentage (%) of {target} in each cluster\\\")\\n\",\n",
    "    \"    fig = px.line(df_relative, x='Clusters', y='Relative Percentage (%)',\\n\",\n",
    "    \"                  color=target, width=800, height=500)\\n\",\n",
    "    \"    fig.update_layout(xaxis=dict(tickmode='array',\\n\",\n",
    "    \"                      tickvals=df['Clusters'].unique()))\\n\",\n",
    "    \"    fig.update_traces(mode='markers+lines')\\n\",\n",
    "    \"    fig.show(renderer='jupyterlab')\\n\",\n",
    "    \"\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"73J7J65v4O_d\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Create a DataFrame that contains best features and Clusters Predictions since we want to analyse the patterns for each cluster.\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"PztdhjGl4Vkg\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_cluster_profile = df_clf.copy()\\n\",\n",
    "    \"df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\\n\",\n",
    "    \"print(df_cluster_profile.shape)\\n\",\n",
    "    \"df_cluster_profile.head(3)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"-mfJRrFc7wzu\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"We want also to analyse Churn levels.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"fSRSNqiF4mnm\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_churn = pd.read_csv(\\\"outputs/datasets/collection/TelcoCustomerChurn.csv\\\").filter(['Churn'])\\n\",\n",
    "    \"df_churn['Churn'] = df_churn['Churn'].astype('object')\\n\",\n",
    "    \"df_churn.head(3)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"KtD0Y3NdJOhm\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"### Cluster profile based on the best features\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"LDhycaSEdORm\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"pd.set_option('display.max_colwidth', None)\\n\",\n",
    "    \"clusters_profile = DescriptionAllClusters(df=pd.concat([df_cluster_profile,df_churn], axis=1), decimal_points=0)\\n\",\n",
    "    \"clusters_profile\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"2SS6CCCb74lH\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"### Clusters distribution across Churn levels & Relative Percentage of Churn in each cluster\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"kwEUdPI2NHOb\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_cluster_vs_churn=  df_churn.copy()\\n\",\n",
    "    \"df_cluster_vs_churn['Clusters'] = X['Clusters']\\n\",\n",
    "    \"cluster_distribution_per_variable(df=df_cluster_vs_churn, target='Churn')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"iEfOvjx8ZhAH\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Fit New Cluster Pipeline with most important features\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"MFno4XnSZlZV\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"In order to reduce feature space, we will study the trade-off between the previous Cluster Pipeline (fitted with all variables) and Pipeline using the variables that are most important to define the clusters from the previous pipeline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"ROH8cre2PHWx\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"best_features_pipeline_all_variables\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"cLOL2zr4Jr68\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Define trade-off and metrics to compare new and previous Cluster Pipeline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"eJyxkSowZ9Cm\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"To evaluate this trade-off we will\\n\",\n",
    "    \"1. Conduct a elbow method and silhouette analysis and check if the same number of clusters is suggested\\n\",\n",
    "    \"2. Fit new cluster pipeline and compare if the predictions from this pipeline are \\\"equivalent\\\" to the predictions from the previous pipeline\\n\",\n",
    "    \"3. Fit a classifier to explain cluster, and check if performance on Train and Test sets is similar to the previous pipeline\\n\",\n",
    "    \"4. Check if the most important features for the classifier are the same from the previous pipeline\\n\",\n",
    "    \"5. Compare if the cluster profile from both pipelines are \\\"equivalent\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"If we are happy to say **yes** for them, we can use a cluster pipeline using the features that best define the clusters from previous pipeline!\\n\",\n",
    "    \"* The **gain** is that in real-time (which is the major purpose of Machine Learning) you will need fewer variables for predicting clusters for your prospects.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"DxyTpm4EJx4s\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Subset data with the most relevant variables\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"lQu9033oZ6r9\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_reduced = df.filter(best_features_pipeline_all_variables)\\n\",\n",
    "    \"df_reduced.head(3)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"_ub9_YoIeaS5\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Rewrite Cluster Pipeline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"PrlQuieZeaS6\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def PipelineCluster():\\n\",\n",
    "    \"    pipeline_base = Pipeline([\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # we update the pipeline, considering only the most important variables from the previous pipeline\\n\",\n",
    "    \"        (\\\"OrdinalCategoricalEncoder\\\", OrdinalEncoder(encoding_method='arbitrary',\\n\",\n",
    "    \"                                                     variables=['OnlineBackup', 'PhoneService'])),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # it doesn't need SmartCorrelation\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"scaler\\\", StandardScaler()),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # No PCA step needed, since we know which features to consider\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"model\\\", KMeans(n_clusters=3, random_state=0)),\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    ])\\n\",\n",
    "    \"    return pipeline_base\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"PipelineCluster()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"D57ncdQ7hBXe\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Apply Elbow Method and Silhouette analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"D0wqQOM3hBXr\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"pipeline_cluster = PipelineCluster()\\n\",\n",
    "    \"pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\\n\",\n",
    "    \"df_analysis = pipeline_analysis.fit_transform(df_reduced)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(df_analysis.shape,'\\\\n', type(df_analysis))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"r_1H05FKhBXs\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Elbow Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"nsAJW4s0hBXt\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from yellowbrick.cluster import KElbowVisualizer\\n\",\n",
    "    \"visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11))\\n\",\n",
    "    \"visualizer.fit(df_analysis) \\n\",\n",
    "    \"visualizer.show() \\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from yellowbrick.cluster import SilhouetteVisualizer\\n\",\n",
    "    \"\\n\",\n",
    "    \"n_cluster_start, n_cluster_stop = 2, 5\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"=== Average Silhouette Score for different number of clusters ===\\\")\\n\",\n",
    "    \"visualizer = KElbowVisualizer(KMeans(random_state=0), k=(\\n\",\n",
    "    \"    n_cluster_start, n_cluster_stop), metric='silhouette')\\n\",\n",
    "    \"visualizer.fit(df_analysis)\\n\",\n",
    "    \"visualizer.show()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"print(\\\"\\\\n\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print(f\\\"=== Silhouette plot for {n_clusters} Clusters ===\\\")\\n\",\n",
    "    \"    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters, random_state=0),\\n\",\n",
    "    \"                                      colors='yellowbrick')\\n\",\n",
    "    \"    visualizer.fit(df_analysis)\\n\",\n",
    "    \"    visualizer.show()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    print(\\\"\\\\n\\\")\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"f_T1gtprhe8W\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Fit New Cluster Pipeline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"ZEtzurhOhe8W\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"We set X as our training set for the cluster. It is a copy of df_reduced\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"BIZRR3wChe8X\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"X = df_reduced.copy()\\n\",\n",
    "    \"print(X.shape)\\n\",\n",
    "    \"X.head(3)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"O-z3ST3nhe8Z\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Fit Cluster pipeline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"AAaEPy3Uhe8Z\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"pipeline_cluster = PipelineCluster()\\n\",\n",
    "    \"pipeline_cluster.fit(X)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"_Pdrt5bdKLDF\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Add cluster predictions to dataset\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"CdFwZk1ihe8b\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"We add a column \\\"`Clusters`\\\" (with the cluster pipeline predictions) to the dataset\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"X['Clusters'] = pipeline_cluster['model'].labels_\\n\",\n",
    "    \"print(X.shape)\\n\",\n",
    "    \"X.head(3)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"wUwR8vCEhe8b\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(f\\\"* Clusters frequencies \\\\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\\\n\\\\n\\\")\\n\",\n",
    "    \"X['Clusters'].value_counts().sort_values().plot(kind='bar')\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"QUxjeMypKOUe\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Compare current cluster predictions to previous cluster predictions\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"w6721kuGiII6\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"We just fitted a new cluster pipeline and want to compare if its predictions are \\\"equivalent\\\" to the previous cluster.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"GAS2dDXQhe8c\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"These are the predictions from the **previous** cluster pipeline - trained with all variables \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"pbVaUAABhe8c\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"cluster_predictions_with_all_variables\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"xUiHzLIwimaD\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"And these are the predictions from **current** cluster pipeline (trained with `['OnlineBackup', 'MonthlyCharges', 'PhoneService']`)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"_kC7cKKCiwD5\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"cluster_predictions_with_best_features = X['Clusters'] \\n\",\n",
    "    \"cluster_predictions_with_best_features\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"3Jt-2n1GidBa\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"We use a confusion matrix to evaluate if the predictions of both pipelines are **\\\"equivalent\\\"**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"tLy37N1TiiSx\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.metrics import confusion_matrix\\n\",\n",
    "    \"print(confusion_matrix(cluster_predictions_with_all_variables, cluster_predictions_with_best_features))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"fcPfalkwmomc\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Fit a classifier, where the target is cluster predictions and features remaining variables\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"_XoL6tuRmomf\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_clf = X.copy()\\n\",\n",
    "    \"print(df_clf.shape)\\n\",\n",
    "    \"df_clf.head(3)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"PSfwpL-Fmomf\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Split Train and Test sets\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"sPyXs27Kmomf\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.model_selection import train_test_split\\n\",\n",
    "    \"\\n\",\n",
    "    \"X_train, X_test, y_train, y_test = train_test_split(\\n\",\n",
    "    \"    df_clf.drop(['Clusters'], axis=1),\\n\",\n",
    "    \"    df_clf['Clusters'],\\n\",\n",
    "    \"    test_size=0.2,\\n\",\n",
    "    \"    random_state=0\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(X_train.shape, X_test.shape)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"0W-cV2ts8A_N\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Rewrite pipeline to explain clusters\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"Lm63GRYP8BIV\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def PipelineClf2ExplainClusters():\\n\",\n",
    "    \"    pipeline_base = Pipeline([\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"OrdinalCategoricalEncoder\\\", OrdinalEncoder(encoding_method='arbitrary',\\n\",\n",
    "    \"                                                     variables=['OnlineBackup', 'PhoneService'])),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # it doesn't need SmartCorrelation\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"scaler\\\", StandardScaler()),\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # we don't consider feature selection step, since we know which features to consider\\n\",\n",
    "    \"\\n\",\n",
    "    \"        (\\\"model\\\", GradientBoostingClassifier(random_state=0)),\\n\",\n",
    "    \"\\n\",\n",
    "    \"    ])\\n\",\n",
    "    \"    return pipeline_base\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"PipelineClf2ExplainClusters()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"lkaBhgjOmomg\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Fit a classifier, where the target is cluster labels and features remaining variables\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"zU6mwsFYmomg\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Create and fit a classifier pipeline to learn the feature importance when defining a cluster\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"w3liI7qjmomg\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"pipeline_clf_cluster = PipelineClf2ExplainClusters()\\n\",\n",
    "    \"pipeline_clf_cluster.fit(X_train,y_train)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"6hCk6Swrmomh\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Evaluate classifier performance on Train and Test Sets\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"ZjXpF0x8momh\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"l-Obn_Hcmomh\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"G07251XWmomh\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Assess Most Important Features\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"IMTUNBYN8fyf\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# since we don't have feature selection step in this pipeline, best_features is Xtrain columns\\n\",\n",
    "    \"best_features = X_train.columns.to_list()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# create a DataFrame to display feature importance\\n\",\n",
    "    \"df_feature_importance = (pd.DataFrame(data={\\n\",\n",
    "    \"    'Feature': best_features,\\n\",\n",
    "    \"    'Importance': pipeline_clf_cluster['model'].feature_importances_})\\n\",\n",
    "    \"    .sort_values(by='Importance', ascending=False)\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"best_features = df_feature_importance['Feature'].to_list()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Most important features statement and plot\\n\",\n",
    "    \"print(f\\\"* These are the {len(best_features)} most important features in descending order. \\\"\\n\",\n",
    "    \"      f\\\"The model was trained on them: \\\\n{df_feature_importance['Feature'].to_list()}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\\n\",\n",
    "    \"plt.show()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"9q1TJSqdI6xK\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Cluster Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"C8EMIqE5I6xP\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Create a DataFrame that contains the best features and Clusters Predictions: we want to analyse the patterns for each cluster.\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"TEg92vdnI6xP\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_cluster_profile = df_clf.copy()\\n\",\n",
    "    \"df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\\n\",\n",
    "    \"df_cluster_profile.head(3)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"789CeA0WI6xQ\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"We want also to analyse Churn levels\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"Nx335aqtI6xR\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_churn = pd.read_csv(\\\"outputs/datasets/collection/TelcoCustomerChurn.csv\\\").filter(['Churn'])\\n\",\n",
    "    \"df_churn['Churn'] = df_churn['Churn'].astype('object')\\n\",\n",
    "    \"df_churn.head(3)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"f-7jYzhtI6xR\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"### Cluster profile on most important features\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"urBmw5HJI6xS\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"pd.set_option('display.max_colwidth', None)\\n\",\n",
    "    \"clusters_profile = DescriptionAllClusters(df= pd.concat([df_cluster_profile,df_churn], axis=1), decimal_points=0)\\n\",\n",
    "    \"clusters_profile\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"LDJRuBBgI6xS\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"### Clusters distribution across Churn levels & Relative Percentage of Churn in each cluster\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"ZjMnAqYKI6xS\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_cluster_vs_churn=  df_churn.copy()\\n\",\n",
    "    \"df_cluster_vs_churn['Clusters'] = X['Clusters']\\n\",\n",
    "    \"cluster_distribution_per_variable(df=df_cluster_vs_churn, target='Churn')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"4xhPLbC4dwXL\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Which pipeline should I deploy?\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"-8qASh5k1jph\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Let's recap the criteria we consider to evaluate the **trade-off**\\n\",\n",
    "    \"1. Conduct an elbow method and silhouette analysis and check if the same number of clusters is suggested.\\n\",\n",
    "    \"2. Fit a new cluster pipeline and compare if the predictions from this pipeline are \\\"equivalent\\\" to the predictions from the previous pipeline.\\n\",\n",
    "    \"3. Fit a classifier to explain cluster and check if performance on Train and Test sets is similar to the previous pipeline.\\n\",\n",
    "    \"4. Check if the most important features for the classifier are the same from the previous pipeline.\\n\",\n",
    "    \"5. Compare if the cluster profile from both pipelines is \\\"equivalent\\\".\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"a4HsuSuqd0g_\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"pipeline_cluster\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"4zxpjktKd1n6\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Push files to Repo\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"5i9X1oOORAQc\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"We will generate the following files\\n\",\n",
    "    \"\\n\",\n",
    "    \"* Cluster Pipeline\\n\",\n",
    "    \"* Train Set\\n\",\n",
    "    \"* Feature importance plot\\n\",\n",
    "    \"* Clusters Description\\n\",\n",
    "    \"* Cluster Silhouette\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"5ySBIrV1Q4cY\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import joblib\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"version = 'v1'\\n\",\n",
    "    \"file_path = f'outputs/ml_pipeline/cluster_analysis/{version}'\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    os.makedirs(name=file_path)\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(e)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"6y9-0fisd5cl\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Cluster pipeline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"Xfv9k5xMd7fv\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"pipeline_cluster\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"IsphnIR84hJ4\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"joblib.dump(value=pipeline_cluster, filename=f\\\"{file_path}/cluster_pipeline.pkl\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"_ORnkwG6d74O\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Train Set\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"QqcwHaVwd9Ff\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(df_reduced.shape)\\n\",\n",
    "    \"df_reduced.head(3)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"M26MiJ9Y485Q\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_reduced.to_csv(f\\\"{file_path}/TrainSet.csv\\\", index=False)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"eX_lcQVXaV0p\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Most important features plot\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"o9datNfsLCVV\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"These are the features that define a cluster\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"FYeoH7fjaV8J\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_feature_importance.plot(kind='bar',x='Feature',y='Importance', figsize=(8,4))\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"sr0cVVQsaZqk\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_feature_importance.plot(kind='bar',x='Feature',y='Importance', figsize=(8,4))\\n\",\n",
    "    \"plt.savefig(f\\\"{file_path}/features_define_cluster.png\\\", bbox_inches='tight', dpi=150)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"GX3Z5ivNd9mw\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Cluster Profile\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"tw-mEnI8d_Bv\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"clusters_profile\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"7G5CsAl738p7\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"clusters_profile.to_csv(f\\\"{file_path}/clusters_profile.csv\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"RObeac1HQq5a\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Cluster silhouette plot\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"visualizer = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick')\\n\",\n",
    "    \"visualizer.fit(df_analysis)\\n\",\n",
    "    \"visualizer.show()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"fig, axes = plt.subplots(figsize=(7,5))\\n\",\n",
    "    \"fig = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick', ax=axes)\\n\",\n",
    "    \"fig.fit(df_analysis)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.savefig(f\\\"{file_path}/clusters_silhouette.png\\\", bbox_inches='tight',dpi=150)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"TeIEUrWkJ-6_\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Good job, clear the cell outputs, run git commands to add, commit and push files to the repo. Next, we will move on to create our dashboard!\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"accelerator\": \"GPU\",\n",
    "  \"colab\": {\n",
    "   \"name\": \"Modeling and Evaluation - Cluster Sklearn.ipynb\",\n",
    "   \"provenance\": [],\n",
    "   \"toc_visible\": true\n",
    "  },\n",
    "  \"interpreter\": {\n",
    "   \"hash\": \"8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce\"\n",
    "  },\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3.8.12 64-bit ('3.8.12': pyenv)\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.12 (default, Sep 27 2022, 15:56:02) \\n[GCC 9.4.0]\"\n",
    "  },\n",
    "  \"orig_nbformat\": 2\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
